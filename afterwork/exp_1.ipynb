{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms import *\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.classes = [d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d)) and not d.startswith('_')]\n",
    "        self.classes.sort()\n",
    "\n",
    "        self.class_to_idx = {self.classes[i]: i for i in range(len(self.classes))}\n",
    "        self.idx_to_class = {idx: c for c, idx in self.class_to_idx.items()} \n",
    "        \n",
    "        self.class_indices = [[] for _ in range(len(self.classes))]\n",
    " \n",
    "        data = []\n",
    "        cur_idx = 0\n",
    "\n",
    "        for data_class in self.classes:\n",
    "            folder_path = os.path.join(folder, data_class)\n",
    "            target = self.class_to_idx[data_class]\n",
    "\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                path = os.path.join(folder_path, file_name)\n",
    "                data.append((path, target))\n",
    "\n",
    "                self.class_indices[target].append(cur_idx)\n",
    "                cur_idx += 1\n",
    "\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.data[index]\n",
    "        data = {'path': path, 'target': target}\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_classes_number(self):\n",
    "        return len(self.classes)\n",
    "\n",
    "    def get_class_from_idx(self, idx):\n",
    "        if idx in self.idx_to_class.keys():\n",
    "            return self.idx_to_class[idx]\n",
    "        return 'unknown'\n",
    "    \n",
    "    def get_idx_from_class(self, c):\n",
    "        if c in self.class_to_idx.keys():\n",
    "            return self.class_to_idx[c]\n",
    "        return -1\n",
    "\n",
    "    def get_class_indices(self):\n",
    "        return self.class_indices\n",
    "\n",
    "    def make_weights_for_balanced_classes(self):\n",
    "        \"\"\"adopted from https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\"\"\"\n",
    "\n",
    "        classes_number = len(self.classes)\n",
    "        classes_size = np.zeros(classes_number)\n",
    "\n",
    "        for i in range(classes_number):\n",
    "            classes_size[i] = len(self.class_indices[i])\n",
    "\n",
    "        total_size = float(sum(classes_size))\n",
    "        weight_per_class = total_size / classes_size\n",
    "\n",
    "        weight = np.zeros(len(self))\n",
    "        for idx, item in enumerate(self.data):\n",
    "            weight[idx] = weight_per_class[item[1]]\n",
    "        return weight\n",
    "\n",
    "class BackgroundNoiseDataset(Dataset):\n",
    "    \"\"\"Dataset for silence / background noise.\"\"\"\n",
    "\n",
    "    def __init__(self, folder, transform=None, sample_rate=16000, sample_length=1):\n",
    "        audio_files = [d for d in os.listdir(folder) if os.path.isfile(os.path.join(folder, d)) and d.endswith('.wav')]\n",
    "        samples = []\n",
    "        for f in audio_files:\n",
    "            path = os.path.join(folder, f)\n",
    "            s, sr = librosa.load(path, sr=sample_rate)\n",
    "            samples.append(s)\n",
    "\n",
    "        samples = np.hstack(samples)\n",
    "        c = int(sample_rate * sample_length)\n",
    "        r = len(samples) // c\n",
    "        self.samples = samples[:r*c].reshape(-1, c)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "        self.path = folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {'samples': self.samples[index], 'sample_rate': self.sample_rate, 'target': 1, 'path': self.path}\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader + Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = nn.functional.pairwise_distance(anchor, positive, 2)\n",
    "        distance_negative = nn.functional.pairwise_distance(anchor, negative, 2)\n",
    "        losses = nn.functional.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TripletLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=30, shuffle=True, num_workers=0):\n",
    "        super().__init__(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "        self.dataset_norm = dataset\n",
    "        self.triplets_in_batch = batch_size // 3\n",
    "        self.class_indices = self.dataset_norm.get_class_indices()\n",
    "        self.labels = np.arange(len(self.class_indices))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        batch=[]\n",
    "\n",
    "        for i in range((len(self.dataset_norm) + self.batch_size - 1) // self.batch_size):\n",
    "            label_pairs = np.zeros((self.triplets_in_batch, 2), dtype=np.int32)\n",
    "\n",
    "            for i in range(self.triplets_in_batch):\n",
    "                anchor_label, negative_label = np.random.choice(self.labels, size=2, replace=False)\n",
    "                label_pairs[i] = [anchor_label, negative_label]\n",
    "            \n",
    "            for anchor_label, negative_label in label_pairs:\n",
    "                anchor_idx, positive_idx = np.random.choice(self.class_indices[anchor_label], 2, replace=False)\n",
    "                negative_idx = np.random.choice(self.class_indices[negative_label])\n",
    "\n",
    "                anchor, anchor_label = self.dataset_norm.__getitem__(anchor_idx)\n",
    "                positive, positive_label = self.dataset_norm.__getitem__(positive_idx)\n",
    "                negative, negative_label = self.dataset_norm.__getitem__(negative_idx)\n",
    "\n",
    "                triplet = torch.stack((anchor, positive, negative))\n",
    "                triplet_labels = torch.stack((anchor_label, positive_label, negative_label))\n",
    "\n",
    "                if len(batch) == 0:\n",
    "                    batch.append(triplet)\n",
    "                    batch.append(triplet_labels)\n",
    "                else:\n",
    "                    batch[0] = torch.vstack((batch[0], triplet))\n",
    "                    batch[1] = torch.hstack((batch[1], triplet_labels))\n",
    "            \n",
    "            yield batch\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifted Structured Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiftedStructuredLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(LiftedStructuredLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        # Compute pairwise distance between features\n",
    "        device = features.device\n",
    "        pairwise_distance = torch.cdist(features, features)\n",
    "\n",
    "        # Create a mask for positive pairs\n",
    "        mask = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))\n",
    "        invert_mask = ~mask\n",
    "        neg_dists = torch.exp((self.margin - pairwise_distance)) * invert_mask\n",
    "\n",
    "        mask *= ((torch.ones(mask.shape[0], mask.shape[0]).to(device) - torch.eye(mask.shape[0], mask.shape[0]).to(device) == 1))\n",
    "\n",
    "        row_exp_sum = torch.sum(neg_dists, dim=1)\n",
    "        table_exp_sum = torch.max(row_exp_sum.unsqueeze(0), row_exp_sum.unsqueeze(1)) + torch.min(row_exp_sum.unsqueeze(0), row_exp_sum.unsqueeze(1))\n",
    "\n",
    "        loss = (pairwise_distance + torch.log(table_exp_sum)) * mask\n",
    "        loss = torch.square(torch.relu(loss))\n",
    "\n",
    "        loss = torch.sum(loss) / torch.sum(mask)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, network, loss_fn, optimizer, dl_train, dl_test, device):\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in (pbar := tqdm.tqdm(range(n_epochs), total=n_epochs, leave=True)):\n",
    "        # Переводим сеть в режим обучения\n",
    "        network.train()\n",
    "\n",
    "        # Итерация обучения сети\n",
    "        for batch in tqdm.tqdm(dl_train, total=len(dl_train), leave=False):\n",
    "            images = batch['input']\n",
    "            images = torch.unsqueeze(images, 1)\n",
    "\n",
    "            labels = batch['target']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            net_out = network(images)\n",
    "            loss = loss_fn(net_out, labels)\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Оцениваем качество модели каждые 3 итерации\n",
    "        if epoch % 3 == 0 or epoch == n_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                iter_number = 0\n",
    "                loss_sum = 0\n",
    "\n",
    "                for batch in tqdm.tqdm(dl_test, total=len(dl_test), leave=False):\n",
    "                    images = batch['input']\n",
    "                    images = torch.unsqueeze(images, 1)\n",
    "\n",
    "                    labels = batch['target']\n",
    "                \n",
    "                    net_out = network(images.to(device))\n",
    "                    losses = loss_fn(net_out, labels.to(device))\n",
    "\n",
    "                    loss_sum += losses.detach().cpu()\n",
    "                    iter_number += 1\n",
    "                \n",
    "                test_losses.append(loss_sum / iter_number)\n",
    "    \n",
    "        pbar.set_description(\n",
    "                        'Loss (Train/Test): {0:.3f}/{1:.3f}\\n'.format(\n",
    "                            train_losses[-1], test_losses[-1]\n",
    "                        )\n",
    "                    )\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSCNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels=1, in_shape=(32, 32), ds_cnn_number=3, ds_cnn_size=64, is_classifier=False, classes_number=0):\n",
    "        super(DSCNN, self).__init__()\n",
    "\n",
    "        self.classes_number = classes_number\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "        ### your code here\n",
    "        self.initial_convolution = self.make_features(in_channels, ds_cnn_size)\n",
    "        self.dscnn_blocks = self.make_dscnn_blocks(ds_cnn_size, ds_cnn_number)\n",
    "        self.pool = self.make_pool(in_shape)\n",
    "\n",
    "        self.classifier = nn.Linear(ds_cnn_size, classes_number)\n",
    "    \n",
    "    def make_features(self, in_channels, out_channels):\n",
    "        layers = []\n",
    "    \n",
    "        layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), padding='same'))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def make_dscnn_blocks(self, ds_cnn_size, ds_cnn_number):\n",
    "        layers = []\n",
    "\n",
    "        for i in range(ds_cnn_number):\n",
    "            layers.append(nn.Conv2d(in_channels=ds_cnn_size, out_channels=ds_cnn_size, kernel_size=3, groups=ds_cnn_size, padding='same'))\n",
    "            layers.append(nn.BatchNorm2d(ds_cnn_size))          \n",
    "            layers.append(torch.nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Conv2d(in_channels=ds_cnn_size, out_channels=ds_cnn_size, kernel_size=1, padding='same'))\n",
    "            layers.append(nn.BatchNorm2d(ds_cnn_size))    \n",
    "            layers.append(torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def make_pool(self, in_shape):\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.AvgPool2d(in_shape))\n",
    "        layers.append(torch.nn.Flatten())\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.initial_convolution(x)\n",
    "        x = self.dscnn_blocks(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        if self.is_classifier:\n",
    "            x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'> cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda', 0)\n",
    "    \n",
    "print(type(device), device)\n",
    "\n",
    "# may benefit if network size/input/output is stable\n",
    "if device == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_noise_path = 'datasets/speech_commands/_background_noise_'\n",
    "train_dataset_path = 'datasets/speech_commands/train'\n",
    "valid_dataset_path = 'datasets/speech_commands/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 32\n",
    "\n",
    "data_aug_transform = Compose([ChangeAmplitude(), ChangeSpeedAndPitchAudio(), FixAudioLength(), ToSTFT(), StretchAudioOnSTFT(), TimeshiftAudioOnSTFT(), FixSTFTDimension()])\n",
    "\n",
    "bg_dataset = BackgroundNoiseDataset(background_noise_path, data_aug_transform)\n",
    "add_bg_noise = AddBackgroundNoiseOnSTFT(bg_dataset)\n",
    "\n",
    "train_feature_transform = Compose([ToMelSpectrogramFromSTFT(n_mels=n_mels), DeleteSTFT(), ToTensor('mel_spectrogram', 'input')])\n",
    "train_dataset = SpeechCommandsDataset(train_dataset_path,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         data_aug_transform,\n",
    "                                         add_bg_noise,\n",
    "                                         train_feature_transform]))\n",
    "\n",
    "valid_feature_transform = Compose([ToSTFT(), ToMelSpectrogramFromSTFT(n_mels=n_mels), ToTensor('mel_spectrogram', 'input')])\n",
    "valid_dataset = SpeechCommandsDataset(valid_dataset_path,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         FixAudioLength(),\n",
    "                                         valid_feature_transform]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "in_shape = (n_mels, n_mels)\n",
    "ds_cnn_number = 3\n",
    "ds_cnn_size = 64\n",
    "\n",
    "is_classifier = True\n",
    "classes_number = train_dataset.get_classes_number()\n",
    "\n",
    "model = DSCNN(in_channels, in_shape, ds_cnn_number, ds_cnn_size, is_classifier, classes_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epoch = 100\n",
    "\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "dl_validation = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19405dcbe92443f59043afa7d34acb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5d6b78104e48c3a59a0be597d32b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbec6e2216b0480e9bba542e25f1ed4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7ffc9b42924c1b81fed2a4c641a994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edf44ce1a6b48efaade67f2aae41531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af76604478dc403a8880c6970dba52dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6d25c8011840bfb2256552bc580ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c12ba03dd744a3d8cecfb7b9a0c2482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2f7a2a63bb4baa91f10e1ef9261891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2303d0147bc54f4c9a4a3d5adc63f207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30299ac0ac9545c08abf6aee1949e1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236ef5efed5c4380b9194d865b3fac60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a545d964ca9a43cfbf76fd701f045881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d811f3317b064f5090135da66e5d4c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6ca54b93694179a6e092997b98af6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5af6221c3f4d4aa88a259e26823199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c484782dda344f193191e3321937be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f67db7c7b14d71bfb5e0f7b6811ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e33a06387d42b489c10a40d297aafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d71363e20046c7939a488ea34c727c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc5b296150445cc8e87c764175f1fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6cec8dee5748f7a13062fe91dc4de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971a0f95105d40199d130b03e4ad03c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bf83921c554d2594081eba9546c16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7078cc4a37a84620b0dd01b1682b73ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c226bee32910406aa20c9734f9e0a9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ec73dee9454b2e80e50054b8b78e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97cff4a8f0745e6ac48caf4d2014399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc1c6e4b71146fe89cc6f586d50a4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d9f8699c964ef68a12ddfae6a69f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f67c31e4c8b46d79f84c862ccd058e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120d4f5db8274e33a3f01cd4810b4740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccfe2779d0e47659229b3b9d0d0b48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf038825cf8d4f199b3db4c242a3c7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a247db08254c6cba8bb6c72543fe5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a1a71b3aaa4f60be7bb566f849676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f6809924ea47c18e266f35a2d7f4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf221095af74bda9563f805a3d3e601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd3261c49b54bd99d6b199d192e240b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2062d13091d4ffa83222be93101d0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4058b292f81b44e68251346f6dbc6ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_losses, test_losses \u001b[39m=\u001b[39m training_loop(n_epoch, model, loss_fn, optimizer, dl_train, dl_validation, device)\n",
      "\u001b[1;32m/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb Cell 24\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m network\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Итерация обучения сети\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(dl_train, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dl_train), leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     images \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(images, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m: path, \u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m: target}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/exp_1.ipynb#X54sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/afterwork/transforms/transforms_stft.py:26\u001b[0m, in \u001b[0;36mToSTFT.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     24\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mn_fft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_fft\n\u001b[1;32m     25\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mhop_length\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhop_length\n\u001b[0;32m---> 26\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mstft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mstft(samples, n_fft\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_fft, hop_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhop_length)\n\u001b[1;32m     27\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mstft_shape\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mstft\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:365\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[1;32m    363\u001b[0m     off_end \u001b[39m=\u001b[39m y_frames_post\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m off_end \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 365\u001b[0m         stft_matrix[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m-\u001b[39moff_end:] \u001b[39m=\u001b[39m fft\u001b[39m.\u001b[39;49mrfft(fft_window \u001b[39m*\u001b[39;49m y_frames_post, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     off_start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mrfft\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/numpy/fft/_pocketfft.py:409\u001b[0m, in \u001b[0;36mrfft\u001b[0;34m(a, n, axis, norm)\u001b[0m\n\u001b[1;32m    407\u001b[0m     n \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mshape[axis]\n\u001b[1;32m    408\u001b[0m inv_norm \u001b[39m=\u001b[39m _get_forward_norm(n, norm)\n\u001b[0;32m--> 409\u001b[0m output \u001b[39m=\u001b[39m _raw_fft(a, n, axis, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, inv_norm)\n\u001b[1;32m    410\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/numpy/fft/_pocketfft.py:73\u001b[0m, in \u001b[0;36m_raw_fft\u001b[0;34m(a, n, axis, is_real, is_forward, inv_norm)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     a \u001b[39m=\u001b[39m swapaxes(a, axis, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     r \u001b[39m=\u001b[39m pfi\u001b[39m.\u001b[39;49mexecute(a, is_real, is_forward, fct)\n\u001b[1;32m     74\u001b[0m     r \u001b[39m=\u001b[39m swapaxes(r, axis, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = training_loop(n_epoch, model, loss_fn, optimizer, dl_train, dl_validation, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handy Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameteres number: 17236\n"
     ]
    }
   ],
   "source": [
    "# Number of model parameters\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameteres number: {pytorch_total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda', 0)\n",
    "\n",
    "print(type(device), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
