{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import models\n",
    "from datasets import *\n",
    "from transforms import *\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('use_gpu', use_gpu)\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "n_mels = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--train-dataset\", type=str, default='datasets/speech_commands/train', help='path of train dataset')\n",
    "parser.add_argument(\"--valid-dataset\", type=str, default='datasets/speech_commands/valid', help='path of validation dataset')\n",
    "parser.add_argument(\"--background-noise\", type=str, default='datasets/speech_commands/train/_background_noise_', help='path of background noise')\n",
    "parser.add_argument(\"--comment\", type=str, default='', help='comment in tensorboard title')\n",
    "parser.add_argument(\"--batch-size\", type=int, default=32, help='batch size')\n",
    "parser.add_argument(\"--dataload-workers-nums\", type=int, default=6, help='number of workers for dataloader')\n",
    "parser.add_argument(\"--weight-decay\", type=float, default=1e-2, help='weight decay')\n",
    "parser.add_argument(\"--optim\", choices=['sgd', 'adam'], default='sgd', help='choices of optimization algorithms')\n",
    "parser.add_argument(\"--learning-rate\", type=float, default=1e-4, help='learning rate for optimization')\n",
    "parser.add_argument(\"--lr-scheduler\", choices=['plateau', 'step'], default='plateau', help='method to adjust learning rate')\n",
    "parser.add_argument(\"--lr-scheduler-patience\", type=int, default=5, help='lr scheduler plateau: Number of epochs with no improvement after which learning rate will be reduced')\n",
    "parser.add_argument(\"--lr-scheduler-step-size\", type=int, default=50, help='lr scheduler step: number of epochs of learning rate decay.')\n",
    "parser.add_argument(\"--lr-scheduler-gamma\", type=float, default=0.1, help='learning rate is multiplied by the gamma to decrease it')\n",
    "parser.add_argument(\"--max-epochs\", type=int, default=70, help='max number of epochs')\n",
    "parser.add_argument(\"--resume\", type=str, help='checkpoint file to resume')\n",
    "parser.add_argument(\"--model\", choices=models.available_models, default=models.available_models[0], help='model of NN')\n",
    "parser.add_argument(\"--input\", choices=['mel32'], default='mel32', help='input of NN')\n",
    "parser.add_argument('--mixup', action='store_true', help='use mixup')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_noise = 'datasets/speech_commands/_background_noise_'\n",
    "train_dataset = 'datasets/speech_commands/train'\n",
    "valid_dataset = 'datasets/speech_commands/validation'\n",
    "batch_size = 32\n",
    "dataload_workers_nums = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_aug_transform = Compose([ChangeAmplitude(), ChangeSpeedAndPitchAudio(), FixAudioLength(), ToSTFT(), StretchAudioOnSTFT(), TimeshiftAudioOnSTFT(), FixSTFTDimension()])\n",
    "bg_dataset = BackgroundNoiseDataset(background_noise, data_aug_transform)\n",
    "add_bg_noise = AddBackgroundNoiseOnSTFT(bg_dataset)\n",
    "train_feature_transform = Compose([ToMelSpectrogramFromSTFT(n_mels=n_mels), DeleteSTFT(), ToTensor('mel_spectrogram', 'input')])\n",
    "train_dataset = SpeechCommandsDataset(train_dataset,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         data_aug_transform,\n",
    "                                         add_bg_noise,\n",
    "                                         train_feature_transform]))\n",
    "\n",
    "valid_feature_transform = Compose([ToMelSpectrogram(n_mels=n_mels), ToTensor('mel_spectrogram', 'input')])\n",
    "valid_dataset = SpeechCommandsDataset(valid_dataset,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         FixAudioLength(),\n",
    "                                         valid_feature_transform]))\n",
    "\n",
    "weights = train_dataset.make_weights_for_balanced_classes()\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
    "                              pin_memory=use_gpu, num_workers=dataload_workers_nums)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              pin_memory=use_gpu, num_workers=dataload_workers_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys(['path', 'target', 'samples', 'sample_rate', 'n_fft', 'hop_length', 'stft_shape', 'mel_spectrogram', 'input'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(len(batch['path']))\n",
    "    print(batch['mel_spectrogram'].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for Google speech commands...\n",
      "epoch   0 with lr=5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45632/45632 [07:34<00:00, 100.31audios/s, loss=2.44878, acc=28.67%]\n",
      "  0%|          | 0/11360 [00:05<?, ?audios/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/afterwork/datasets/speech_commands_dataset.py\", line 48, in __getitem__\n    data = self.transform(data)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/afterwork/transforms/transforms_wav.py\", line 140, in __call__\n    s = librosa.feature.melspectrogram(S=samples, sr=sample_rate, n_mels=self.n_mels)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/librosa/feature/spectral.py\", line 2144, in melspectrogram\n    S, n_fft = _spectrogram(\n               ^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py\", line 2826, in _spectrogram\n    if n_fft is None or n_fft // 2 + 1 != S.shape[-2]:\n                                          ~~~~~~~^^^^\nIndexError: tuple index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, \u001b[39m70\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=220'>221</a>\u001b[0m     \u001b[39m# if args.lr_scheduler == 'step':\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=221'>222</a>\u001b[0m     \u001b[39m#     lr_scheduler.step()\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m     train(epoch)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m     epoch_loss \u001b[39m=\u001b[39m valid(epoch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep(metrics\u001b[39m=\u001b[39mepoch_loss)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m     time_elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m since\n",
      "\u001b[1;32m/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(valid_dataloader, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maudios\u001b[39m\u001b[39m\"\u001b[39m, unit_scale\u001b[39m=\u001b[39mvalid_dataloader\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     inputs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vasili/Desktop/msu_3_coursework/afterwork/test.ipynb#X11sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(inputs, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/afterwork/datasets/speech_commands_dataset.py\", line 48, in __getitem__\n    data = self.transform(data)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/afterwork/transforms/transforms_wav.py\", line 140, in __call__\n    s = librosa.feature.melspectrogram(S=samples, sr=sample_rate, n_mels=self.n_mels)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/librosa/feature/spectral.py\", line 2144, in melspectrogram\n    S, n_fft = _spectrogram(\n               ^^^^^^^^^^^^^\n  File \"/Users/vasili/Desktop/msu_3_coursework/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py\", line 2826, in _spectrogram\n    if n_fft is None or n_fft // 2 + 1 != S.shape[-2]:\n                                          ~~~~~~~^^^^\nIndexError: tuple index out of range\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "# print('use_gpu', use_gpu)\n",
    "# if use_gpu:\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# n_mels = 32\n",
    "# if args.input == 'mel40':\n",
    "#     n_mels = 40\n",
    "\n",
    "# data_aug_transform = Compose([ChangeAmplitude(), ChangeSpeedAndPitchAudio(), FixAudioLength(), ToSTFT(), StretchAudioOnSTFT(), TimeshiftAudioOnSTFT(), FixSTFTDimension()])\n",
    "# bg_dataset = BackgroundNoiseDataset(args.background_noise, data_aug_transform)\n",
    "# add_bg_noise = AddBackgroundNoiseOnSTFT(bg_dataset)\n",
    "# train_feature_transform = Compose([ToMelSpectrogramFromSTFT(n_mels=n_mels), DeleteSTFT(), ToTensor('mel_spectrogram', 'input')])\n",
    "# train_dataset = SpeechCommandsDataset(args.train_dataset,\n",
    "#                                 Compose([LoadAudio(),\n",
    "#                                          data_aug_transform,\n",
    "#                                          add_bg_noise,\n",
    "#                                          train_feature_transform]))\n",
    "\n",
    "# valid_feature_transform = Compose([ToMelSpectrogram(n_mels=n_mels), ToTensor('mel_spectrogram', 'input')])\n",
    "# valid_dataset = SpeechCommandsDataset(args.valid_dataset,\n",
    "#                                 Compose([LoadAudio(),\n",
    "#                                          FixAudioLength(),\n",
    "#                                          valid_feature_transform]))\n",
    "\n",
    "# weights = train_dataset.make_weights_for_balanced_classes()\n",
    "# sampler = WeightedRandomSampler(weights, len(weights))\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler,\n",
    "#                               pin_memory=use_gpu, num_workers=args.dataload_workers_nums)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "#                               pin_memory=use_gpu, num_workers=args.dataload_workers_nums)\n",
    "\n",
    "# a name used to save checkpoints etc.\n",
    "full_name = 'checkpoint'\n",
    "# full_name = '%s_%s_%s_bs%d_lr%.1e_wd%.1e' % (args.model, args.optim, args.lr_scheduler, args.batch_size, args.learning_rate, args.weight_decay)\n",
    "# if args.comment:\n",
    "#     full_name = '%s_%s' % (full_name, args.comment)\n",
    "\n",
    "model = models.create_model(model_name='ds_cnn', num_classes=20, in_channels=1)\n",
    "\n",
    "if use_gpu:\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# if args.optim == 'sgd':\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "# else:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "start_timestamp = int(time.time()*1000)\n",
    "start_epoch = 0\n",
    "best_accuracy = 0\n",
    "best_loss = 1e100\n",
    "global_step = 0\n",
    "\n",
    "# if args.resume:\n",
    "#     print(\"resuming a checkpoint\")\n",
    "#     checkpoint = torch.load(args.resume)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "#     model.float()\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "#     best_accuracy = checkpoint.get('accuracy', best_accuracy)\n",
    "#     best_loss = checkpoint.get('loss', best_loss)\n",
    "#     start_epoch = checkpoint.get('epoch', start_epoch)\n",
    "#     global_step = checkpoint.get('step', global_step)\n",
    "\n",
    "#     del checkpoint  # reduce memory\n",
    "\n",
    "# if args.lr_scheduler == 'plateau':\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1)\n",
    "# else:\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_scheduler_step_size, gamma=args.lr_scheduler_gamma, last_epoch=start_epoch-1)\n",
    "\n",
    "def get_lr():\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "writer = SummaryWriter(comment=('_speech_commands_' + full_name))\n",
    "\n",
    "def train(epoch):\n",
    "    global global_step\n",
    "\n",
    "    print(\"epoch %3d with lr=%.02e\" % (epoch, get_lr()))\n",
    "    phase = 'train'\n",
    "    writer.add_scalar('%s/learning_rate' % phase,  get_lr(), epoch)\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_dataloader, unit=\"audios\", unit_scale=train_dataloader.batch_size)\n",
    "    for batch in pbar:\n",
    "        inputs = batch['input']\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        targets = batch['target']\n",
    "\n",
    "        # if args.mixup:\n",
    "        #     inputs, targets = mixup(inputs, targets, num_classes=len(CLASSES))\n",
    "\n",
    "        inputs = Variable(inputs, requires_grad=True)\n",
    "        targets = Variable(targets, requires_grad=False)\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "        # forward/backward\n",
    "        outputs = model(inputs)\n",
    "        # if args.mixup:\n",
    "        #     loss = mixup_cross_entropy_loss(outputs, targets)\n",
    "        # else:\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "        running_loss += loss.data.item()\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        # if args.mixup:\n",
    "        #     targets = batch['target']\n",
    "        #     targets = Variable(targets, requires_grad=False).cuda(non_blocking=True)\n",
    "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        writer.add_scalar('%s/loss' % phase, loss.data.item(), global_step)\n",
    "\n",
    "        # update the progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': \"%.05f\" % (running_loss / it),\n",
    "            'acc': \"%.02f%%\" % (100*correct/total)\n",
    "        })\n",
    "\n",
    "    accuracy = correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    writer.add_scalar('%s/accuracy' % phase, 100*accuracy, epoch)\n",
    "    writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
    "\n",
    "def valid(epoch):\n",
    "    global best_accuracy, best_loss, global_step\n",
    "\n",
    "    phase = 'valid'\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(valid_dataloader, unit=\"audios\", unit_scale=valid_dataloader.batch_size)\n",
    "    for batch in pbar:\n",
    "        inputs = batch['input']\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        targets = batch['target']\n",
    "\n",
    "        inputs = Variable(inputs, volatile = True)\n",
    "        targets = Variable(targets, requires_grad=False)\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "        running_loss += loss.data.item()\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        writer.add_scalar('%s/loss' % phase, loss.data.item(), global_step)\n",
    "\n",
    "        # update the progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': \"%.05f\" % (running_loss / it),\n",
    "            'acc': \"%.02f%%\" % (100*correct/total)\n",
    "        })\n",
    "\n",
    "    accuracy = correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    writer.add_scalar('%s/accuracy' % phase, 100*accuracy, epoch)\n",
    "    writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': global_step,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(checkpoint, 'checkpoints/best-loss-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-loss.pth' % (start_timestamp, full_name))\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(checkpoint, 'checkpoints/best-acc-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-acc.pth' % (start_timestamp, full_name))\n",
    "\n",
    "    torch.save(checkpoint, 'checkpoints/last-speech-commands-checkpoint.pth')\n",
    "    del checkpoint  # reduce memory\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "print(\"training for Google speech commands...\")\n",
    "since = time.time()\n",
    "for epoch in range(start_epoch, 70):\n",
    "    # if args.lr_scheduler == 'step':\n",
    "    #     lr_scheduler.step()\n",
    "\n",
    "    train(epoch)\n",
    "    epoch_loss = valid(epoch)\n",
    "\n",
    "    lr_scheduler.step(metrics=epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    time_str = 'total time elapsed: {:.0f}h {:.0f}m {:.0f}s '.format(time_elapsed // 3600, time_elapsed % 3600 // 60, time_elapsed % 60)\n",
    "    print(\"%s, best accuracy: %.02f%%, best loss %f\" % (time_str, 100*best_accuracy, best_loss))\n",
    "print(\"finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
