{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import models\n",
    "from datasets import *\n",
    "from transforms import *\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('use_gpu', use_gpu)\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "n_mels = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--train-dataset\", type=str, default='datasets/speech_commands/train', help='path of train dataset')\n",
    "parser.add_argument(\"--valid-dataset\", type=str, default='datasets/speech_commands/valid', help='path of validation dataset')\n",
    "parser.add_argument(\"--background-noise\", type=str, default='datasets/speech_commands/train/_background_noise_', help='path of background noise')\n",
    "parser.add_argument(\"--comment\", type=str, default='', help='comment in tensorboard title')\n",
    "parser.add_argument(\"--batch-size\", type=int, default=32, help='batch size')\n",
    "parser.add_argument(\"--dataload-workers-nums\", type=int, default=6, help='number of workers for dataloader')\n",
    "parser.add_argument(\"--weight-decay\", type=float, default=1e-2, help='weight decay')\n",
    "parser.add_argument(\"--optim\", choices=['sgd', 'adam'], default='sgd', help='choices of optimization algorithms')\n",
    "parser.add_argument(\"--learning-rate\", type=float, default=1e-4, help='learning rate for optimization')\n",
    "parser.add_argument(\"--lr-scheduler\", choices=['plateau', 'step'], default='plateau', help='method to adjust learning rate')\n",
    "parser.add_argument(\"--lr-scheduler-patience\", type=int, default=5, help='lr scheduler plateau: Number of epochs with no improvement after which learning rate will be reduced')\n",
    "parser.add_argument(\"--lr-scheduler-step-size\", type=int, default=50, help='lr scheduler step: number of epochs of learning rate decay.')\n",
    "parser.add_argument(\"--lr-scheduler-gamma\", type=float, default=0.1, help='learning rate is multiplied by the gamma to decrease it')\n",
    "parser.add_argument(\"--max-epochs\", type=int, default=70, help='max number of epochs')\n",
    "parser.add_argument(\"--resume\", type=str, help='checkpoint file to resume')\n",
    "parser.add_argument(\"--model\", choices=models.available_models, default=models.available_models[0], help='model of NN')\n",
    "parser.add_argument(\"--input\", choices=['mel32'], default='mel32', help='input of NN')\n",
    "parser.add_argument('--mixup', action='store_true', help='use mixup')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_noise = 'datasets/speech_commands/_background_noise_'\n",
    "train_dataset = 'datasets/speech_commands/train'\n",
    "valid_dataset = 'datasets/speech_commands/validation'\n",
    "batch_size = 32\n",
    "dataload_workers_nums = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_aug_transform = Compose([ChangeAmplitude(), ChangeSpeedAndPitchAudio(), FixAudioLength(), ToSTFT(), StretchAudioOnSTFT(), TimeshiftAudioOnSTFT(), FixSTFTDimension()])\n",
    "bg_dataset = BackgroundNoiseDataset(background_noise, data_aug_transform)\n",
    "add_bg_noise = AddBackgroundNoiseOnSTFT(bg_dataset)\n",
    "train_feature_transform = Compose([ToMelSpectrogramFromSTFT(n_mels=n_mels), DeleteSTFT(), ToTensor('mel_spectrogram', 'input')])\n",
    "train_dataset = SpeechCommandsDataset(train_dataset,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         data_aug_transform,\n",
    "                                         add_bg_noise,\n",
    "                                         train_feature_transform]))\n",
    "\n",
    "valid_feature_transform = Compose([ToMelSpectrogram(n_mels=n_mels), ToTensor('mel_spectrogram', 'input')])\n",
    "valid_dataset = SpeechCommandsDataset(valid_dataset,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         FixAudioLength(),\n",
    "                                         valid_feature_transform]))\n",
    "\n",
    "weights = train_dataset.make_weights_for_balanced_classes()\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
    "                              pin_memory=use_gpu, num_workers=dataload_workers_nums)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              pin_memory=use_gpu, num_workers=dataload_workers_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys(['path', 'target', 'samples', 'sample_rate', 'n_fft', 'hop_length', 'stft_shape', 'mel_spectrogram', 'input'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(len(batch['path']))\n",
    "    print(batch['mel_spectrogram'].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for Google speech commands...\n",
      "epoch   0 with lr=5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 24640/45632 [04:01<03:19, 105.08audios/s, loss=2.76194, acc=17.63%]"
     ]
    }
   ],
   "source": [
    "\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "# print('use_gpu', use_gpu)\n",
    "# if use_gpu:\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# n_mels = 32\n",
    "# if args.input == 'mel40':\n",
    "#     n_mels = 40\n",
    "\n",
    "# data_aug_transform = Compose([ChangeAmplitude(), ChangeSpeedAndPitchAudio(), FixAudioLength(), ToSTFT(), StretchAudioOnSTFT(), TimeshiftAudioOnSTFT(), FixSTFTDimension()])\n",
    "# bg_dataset = BackgroundNoiseDataset(args.background_noise, data_aug_transform)\n",
    "# add_bg_noise = AddBackgroundNoiseOnSTFT(bg_dataset)\n",
    "# train_feature_transform = Compose([ToMelSpectrogramFromSTFT(n_mels=n_mels), DeleteSTFT(), ToTensor('mel_spectrogram', 'input')])\n",
    "# train_dataset = SpeechCommandsDataset(args.train_dataset,\n",
    "#                                 Compose([LoadAudio(),\n",
    "#                                          data_aug_transform,\n",
    "#                                          add_bg_noise,\n",
    "#                                          train_feature_transform]))\n",
    "\n",
    "# valid_feature_transform = Compose([ToMelSpectrogram(n_mels=n_mels), ToTensor('mel_spectrogram', 'input')])\n",
    "# valid_dataset = SpeechCommandsDataset(args.valid_dataset,\n",
    "#                                 Compose([LoadAudio(),\n",
    "#                                          FixAudioLength(),\n",
    "#                                          valid_feature_transform]))\n",
    "\n",
    "# weights = train_dataset.make_weights_for_balanced_classes()\n",
    "# sampler = WeightedRandomSampler(weights, len(weights))\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler,\n",
    "#                               pin_memory=use_gpu, num_workers=args.dataload_workers_nums)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "#                               pin_memory=use_gpu, num_workers=args.dataload_workers_nums)\n",
    "\n",
    "# a name used to save checkpoints etc.\n",
    "full_name = 'checkpoint'\n",
    "# full_name = '%s_%s_%s_bs%d_lr%.1e_wd%.1e' % (args.model, args.optim, args.lr_scheduler, args.batch_size, args.learning_rate, args.weight_decay)\n",
    "# if args.comment:\n",
    "#     full_name = '%s_%s' % (full_name, args.comment)\n",
    "\n",
    "model = models.create_model(model_name='ds_cnn', num_classes=20, in_channels=1)\n",
    "\n",
    "if use_gpu:\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# if args.optim == 'sgd':\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "# else:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "start_timestamp = int(time.time()*1000)\n",
    "start_epoch = 0\n",
    "best_accuracy = 0\n",
    "best_loss = 1e100\n",
    "global_step = 0\n",
    "\n",
    "# if args.resume:\n",
    "#     print(\"resuming a checkpoint\")\n",
    "#     checkpoint = torch.load(args.resume)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "#     model.float()\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "#     best_accuracy = checkpoint.get('accuracy', best_accuracy)\n",
    "#     best_loss = checkpoint.get('loss', best_loss)\n",
    "#     start_epoch = checkpoint.get('epoch', start_epoch)\n",
    "#     global_step = checkpoint.get('step', global_step)\n",
    "\n",
    "#     del checkpoint  # reduce memory\n",
    "\n",
    "# if args.lr_scheduler == 'plateau':\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1)\n",
    "# else:\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_scheduler_step_size, gamma=args.lr_scheduler_gamma, last_epoch=start_epoch-1)\n",
    "\n",
    "def get_lr():\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "writer = SummaryWriter(comment=('_speech_commands_' + full_name))\n",
    "\n",
    "def train(epoch):\n",
    "    global global_step\n",
    "\n",
    "    print(\"epoch %3d with lr=%.02e\" % (epoch, get_lr()))\n",
    "    phase = 'train'\n",
    "    writer.add_scalar('%s/learning_rate' % phase,  get_lr(), epoch)\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_dataloader, unit=\"audios\", unit_scale=train_dataloader.batch_size)\n",
    "    for batch in pbar:\n",
    "        inputs = batch['input']\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        targets = batch['target']\n",
    "\n",
    "        # if args.mixup:\n",
    "        #     inputs, targets = mixup(inputs, targets, num_classes=len(CLASSES))\n",
    "\n",
    "        inputs = Variable(inputs, requires_grad=True)\n",
    "        targets = Variable(targets, requires_grad=False)\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "        # forward/backward\n",
    "        outputs = model(inputs)\n",
    "        # if args.mixup:\n",
    "        #     loss = mixup_cross_entropy_loss(outputs, targets)\n",
    "        # else:\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "        running_loss += loss.data.item()\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        # if args.mixup:\n",
    "        #     targets = batch['target']\n",
    "        #     targets = Variable(targets, requires_grad=False).cuda(non_blocking=True)\n",
    "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        writer.add_scalar('%s/loss' % phase, loss.data.item(), global_step)\n",
    "\n",
    "        # update the progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': \"%.05f\" % (running_loss / it),\n",
    "            'acc': \"%.02f%%\" % (100*correct/total)\n",
    "        })\n",
    "\n",
    "    accuracy = correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    writer.add_scalar('%s/accuracy' % phase, 100*accuracy, epoch)\n",
    "    writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
    "\n",
    "def valid(epoch):\n",
    "    global best_accuracy, best_loss, global_step\n",
    "\n",
    "    phase = 'valid'\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(valid_dataloader, unit=\"audios\", unit_scale=valid_dataloader.batch_size)\n",
    "    for batch in pbar:\n",
    "        inputs = batch['input']\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        targets = batch['target']\n",
    "\n",
    "        inputs = Variable(inputs, volatile = True)\n",
    "        targets = Variable(targets, requires_grad=False)\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "        running_loss += loss.data.item()\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        writer.add_scalar('%s/loss' % phase, loss.data.item(), global_step)\n",
    "\n",
    "        # update the progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': \"%.05f\" % (running_loss / it),\n",
    "            'acc': \"%.02f%%\" % (100*correct/total)\n",
    "        })\n",
    "\n",
    "    accuracy = correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    writer.add_scalar('%s/accuracy' % phase, 100*accuracy, epoch)\n",
    "    writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': global_step,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(checkpoint, 'checkpoints/best-loss-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-loss.pth' % (start_timestamp, full_name))\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(checkpoint, 'checkpoints/best-acc-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-acc.pth' % (start_timestamp, full_name))\n",
    "\n",
    "    torch.save(checkpoint, 'checkpoints/last-speech-commands-checkpoint.pth')\n",
    "    del checkpoint  # reduce memory\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "print(\"training for Google speech commands...\")\n",
    "since = time.time()\n",
    "for epoch in range(start_epoch, 70):\n",
    "    # if args.lr_scheduler == 'step':\n",
    "    #     lr_scheduler.step()\n",
    "\n",
    "    train(epoch)\n",
    "    epoch_loss = valid(epoch)\n",
    "\n",
    "    lr_scheduler.step(metrics=epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    time_str = 'total time elapsed: {:.0f}h {:.0f}m {:.0f}s '.format(time_elapsed // 3600, time_elapsed % 3600 // 60, time_elapsed % 60)\n",
    "    print(\"%s, best accuracy: %.02f%%, best loss %f\" % (time_str, 100*best_accuracy, best_loss))\n",
    "print(\"finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
